{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"text-align: center; font-size: 36px; color: #3498db; font-weight: bold;\">Prosody Application</h1>\n",
    "## <h2 style=\"text-align: center; font-size: 28px; color: #2ecc71; font-weight: bold;\">Prosody Active Learning</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "import pyaudio\n",
    "import wave\n",
    "import tensorflow as tf \n",
    "import os \n",
    "import pickle\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.effects\n",
    "from joblib import Parallel, delayed\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "# Cloud Folder\n",
    "import os \n",
    "if not os.path.exists(\"Models\"):\n",
    "    import gdown\n",
    "    folder_url =r\"https://drive.google.com/drive/folders/1EZL6Ejoa5GH8DzoZcjvRAonlgWznEh14?usp=drive_link\" \n",
    "    gdown.download_folder(folder_url)\n",
    "##########################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Prosody Model has been loaded\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "# loding the model, weights, scaler and encoder\n",
    "###############################################\n",
    "\n",
    "prosody_model = tf.keras.models.load_model(r'./Models/Prosody_Model.keras')\n",
    "print (\"Prosody Model has been loaded\")\n",
    "\n",
    "# loding the Scaler\n",
    "with open(r\"./Models/Prosody_Scaler.pickle\", 'rb') as f:\n",
    "    prosody_scaler = pickle.load(f)\n",
    "\n",
    "# loding the Encoder    \n",
    "with open(r\"./Models/Prosody_Encoder.pickle\", 'rb') as f:\n",
    "    prosody_encoder = pickle.load(f)\n",
    "    \n",
    "OUTPUT_FILE=r\"./Output/input_voice.wav\"\n",
    "Output_folder= r\"./Output\"\n",
    "if not os.path.exists(Output_folder):    \n",
    "    os.makedirs(Output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['Noise', 'angry', 'disgust', 'fear', 'happy', 'neutral', 'sad',\n",
       "        'surprise'], dtype=object)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prosody_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# geting the features from the voice\n",
    "#####################################\n",
    "\n",
    "## ZCR: Zero Crossing Rate: The rate of sign changes of the signal during the duration of a particular frame\n",
    "def zcr(data, frame_length, hop_length):\n",
    "    zcr = librosa.feature.zero_crossing_rate(y=data, frame_length=frame_length, hop_length=hop_length)\n",
    "    return np.squeeze(zcr)\n",
    "\n",
    "## RMS: root mean square value\n",
    "def rmse(data, frame_length=2048, hop_length=512):\n",
    "    rmse = librosa.feature.rms(y=data, frame_length=frame_length, hop_length=hop_length)\n",
    "    return np.squeeze(rmse)\n",
    "\n",
    "## MFCC: Mel Frequency Cepstral Coefficients form a cepstral representation where the frequency bands are not linear but distributed according to the mel-scale\n",
    "def mfcc(data,sr,frame_length=2048,hop_length=512,flatten:bool=True):\n",
    "    mfcc=librosa.feature.mfcc(y=data,sr=sr)\n",
    "    return np.squeeze(mfcc.T)if not flatten else np.ravel(mfcc.T)\n",
    "\n",
    "## Extraxing the features\n",
    "def extract_features(data, sr=22050, frame_length=2048, hop_length=512):\n",
    "    result = np.hstack((\n",
    "                        zcr(data, frame_length, hop_length),\n",
    "                        rmse(data, frame_length, hop_length),\n",
    "                        mfcc(data, sr, frame_length, hop_length)\n",
    "                        ))\n",
    "    return result\n",
    "\n",
    "###############################\n",
    "# features extraxtion function\n",
    "###############################\n",
    "def get_features(path):\n",
    "    \n",
    "    data, sr= librosa.load(path, duration=2.5, offset=0) # Extract for 2.5 seconds\n",
    "    \n",
    "    result=extract_features(data).reshape((1,-1))\n",
    "    result = prosody_scaler.transform(result)  # Scaler\n",
    "    \n",
    "    return result\n",
    "\n",
    "######################\n",
    "# Prediction function\n",
    "######################\n",
    "def prediction(path):\n",
    "    result = get_features(path)\n",
    "    prediction = prosody_model.predict(result)\n",
    "    y_prediction = prosody_encoder.inverse_transform(prediction.reshape(1, -1))\n",
    "    predicted_class = y_prediction[0][0]\n",
    "    # class probabilities\n",
    "    predicted_probs = prediction[0]\n",
    "    # class names from encoder\n",
    "    class_names = prosody_encoder.categories_[0]\n",
    "    ''''\n",
    "    # Print predicted class and probabilities for all classes\n",
    "    print(\"Predictions for all classes:\")\n",
    "    for label, prob in zip(class_names, predicted_probs):\n",
    "        print(f\"{label}: {prob*100:.2f}%\")\n",
    "    '''\n",
    "    return predicted_class, predicted_probs\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################\n",
    "# Database\n",
    "##########################\n",
    "\n",
    "def df_database_function(database_folder):\n",
    "    \n",
    "    datagrams = []\n",
    "    for filename in os.listdir(database_folder):\n",
    "        if filename.endswith('.wav'):\n",
    "            emotion = filename.split('_')[0]\n",
    "            file_path = os.path.join(database_folder, filename)\n",
    "            datagram = {'path': file_path, 'Emotions': emotion}\n",
    "            datagrams.append(datagram)\n",
    "            \n",
    "    df0 = pd.DataFrame(datagrams)\n",
    "\n",
    "    ##########################\n",
    "    # Extraction process\n",
    "    ###########################\n",
    "    def process_feature(path, emotion):\n",
    "        features = get_features(path)\n",
    "        x = features.flatten()\n",
    "        y = emotion\n",
    "        return x, y\n",
    "    \n",
    "    paths = df0.path\n",
    "    emotions = df0.Emotions\n",
    "    ##########################%%%%%% this parallel loop is chaotic in a sensse that things no longer remian in a particular order as in the df0!################################\n",
    "    # parallel loop \n",
    "    results = Parallel(n_jobs=-1)(delayed(process_feature)(path, emotion) for (path, emotion) in zip(paths, emotions))\n",
    "    X = []\n",
    "    Y = []\n",
    "    for x,y in results:\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    df = pd.DataFrame(X)\n",
    "    df['Emotions'] = Y\n",
    "    \n",
    "    return df\n",
    "\n",
    "database_folder = r\"./new_recordings\"\n",
    "df_new = df_database_function(database_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "def get_date_string():\n",
    "    current_datetime = datetime.now(timezone.utc)\n",
    "\n",
    "    # Format the datetime as desired\n",
    "    formatted_datetime = current_datetime.strftime('%Y_%m_%d_%H-%M')\n",
    "    return formatted_datetime\n",
    "\n",
    "def get_latest_experiment(experiments_dir = r\"tmp\"):\n",
    "\n",
    "    # List all folders in the experiments directory\n",
    "    experiment_folders = [folder for folder in os.listdir(experiments_dir)]\n",
    "    if not experiment_folders: #check if empty\n",
    "        return \"exp_\" + get_date_string()\n",
    "    \n",
    "    # Parse folder names and extract datetime information\n",
    "    parsed_folders = []\n",
    "    for folder_name in experiment_folders:\n",
    "        try:\n",
    "            folder_datetime = datetime.strptime(folder_name, 'exp_%Y_%m_%d_%H-%M')\n",
    "            parsed_folders.append((folder_datetime, folder_name))\n",
    "        except ValueError:\n",
    "            print(ValueError)\n",
    "            # Skip folders with names not matching the expected format\n",
    "            pass\n",
    "\n",
    "    # Sort the parsed folders based on datetime\n",
    "    sorted_folders = sorted(parsed_folders, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Retrieve the latest folder name\n",
    "    latest_folder = sorted_folders[0][1] if sorted_folders else None\n",
    "\n",
    "    print(\"Latest experiment folder:\", latest_folder)\n",
    "\n",
    "    return latest_folder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 5.9775 - accuracy: 0.1429 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 4.6712 - accuracy: 0.2857 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 364ms/step - loss: 2.1347 - accuracy: 0.5714 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 0.7096 - accuracy: 0.7143 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7905 - accuracy: 0.5714 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 0.5110 - accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 1s 511ms/step - loss: 0.1181 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 399ms/step - loss: 0.0319 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 481ms/step - loss: 0.0100 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "1/1 [==============================] - 1s 906ms/step - loss: 0.0097 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.0036 - accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0099 - accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "1/1 [==============================] - 1s 867ms/step - loss: 0.0035 - accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0165 - accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.0027 - accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0056 - accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0219 - accuracy: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0031 - accuracy: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0039 - accuracy: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 1s 648ms/step - loss: 0.0018 - accuracy: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0060 - accuracy: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0066 - accuracy: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0029 - accuracy: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0154 - accuracy: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0023 - accuracy: 1.0000 - lr: 3.1250e-05\n"
     ]
    }
   ],
   "source": [
    "new_experiment = True\n",
    "\n",
    "\n",
    "def model_finetune(df):\n",
    "    X= df.iloc[: ,:-1]\n",
    "    y = df['Emotions']\n",
    "    X = prosody_scaler.fit_transform(X)\n",
    "    y = prosody_encoder.transform(y.to_numpy().reshape(-1, 1)).toarray()  # no fit_transform as it will remove the previous state and will only have as many classes as in the current dataset\n",
    "    X_cnn = np.expand_dims(X, axis=2)\n",
    "    \n",
    "    if new_experiment: \n",
    "        exp_dir = f\"tmp/exp_{get_date_string()}\"\n",
    "    else: \n",
    "        exp_dir = get_latest_experiment(experiments_dir=r\"tmp\")\n",
    "        \n",
    "    # checkpoint_path = os.path.join(exp_dir, r\"ckpts/Model_{epoch:02d}-{accuracy:.2f}-{loss:.4f}.keras\")\n",
    "    checkpoint_path = exp_dir + r\"/ckpts/Model_{epoch:02d}-{accuracy:.2f}-{loss:.4f}.keras\"\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(checkpoint_path, monitor='loss', save_best_only=True, save_weights_only=False)\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='loss', mode='auto', patience=5, restore_best_weights=True)\n",
    "\n",
    "    lr_reduction = ReduceLROnPlateau(monitor='accuracy', patience=3, verbose=1, factor=0.5, min_lr=0.000001)\n",
    "    optimiser = Adam(learning_rate= 1e-3)\n",
    "    prosody_model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = prosody_model.fit(X_cnn, y, epochs=50, batch_size=64, callbacks=[early_stop, lr_reduction, model_checkpoint])\n",
    "    history_path = exp_dir + '/history.pkl'\n",
    "    ###save the history\n",
    "    with open(history_path, 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "    \n",
    "model_finetune(df_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kali",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
